---
title: NYC Property Sales Project
author: Kuntal Bhar
output:
  pdf_document: default
  html_document: default
---

# INTRODUCTIONS

As we know New York City real state is among the most expensive cities in the world.We uses NYC Property Sales Dataset provided by the New York City Department of Finance to do our analysis.

This dataset have record of every building or building unit (apartment, etc.) sold in the New York City property market over a 12-month period from  September 1, 2016 to August 31, 2017. We try discover about New York City real estate by looking at a year's worth of raw transaction records and  spot trends in the market, or build a model that predicts sale value in the future.

## Objective

We do analysis on 
      1.Data analysis that includes data import ,analysing the data and its variable like Borough, Neighborhood, Age of the building/property, Size of property and type of building), cleaning of data if needed, data preview etc. 
      2.Descriptive, Visualization and Predictive Analysis various categories results like Most In-Demand Borough, Most In-Demand Neighborhood, Preferred Buildings, Property sizes, Age of the buildings and spot trends in the market.
      

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
#Packages used
knitr::opts_chunk$set(echo = FALSE)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(magrittr)) install.packages("magrittr", repos = "http://cran.us.r-project.org")
if(!require(modelr)) install.packages("modelr", repos = "http://cran.us.r-project.org")
if(!require(plotly)) install.packages("plotly", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(formattable)) install.packages("formattable", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(broom)) install.packages("broom", repos = "http://cran.us.r-project.org")


```


# DATASET ANALYSIS

We explore the data provided by New York City Department of Finance. We do various steps here for preparing the data. Steps include data import, data cleaning, data preview, data details.

The NYC Property Sales Dataset is a record of every building or apartment unit that was sold in the NYC Property market over a 12 month period of 5 boroughs in NYC - Manhattan, the Bronx, Queens, Brookyln and Staten Island.
For furthe detail please sheck the refrence section at the bottom

Here we prepare the data. Each sation section contains the logic and the steps performed in bringing the data to a form suitable for statistical analysis. Each section explains the related steps for importing and cleaning the data. 

The dataset was downloaded nyc-rolling-sales.csv from Kaggle www.kaggle.com/new-york-city/nyc-property-sales

NOTE:We can downloaded the file nyc-rolling-sales.csv dataset was manualy from Kagglewww.kaggle.com/new-york-city/nyc-property-sales and save it in save the file nyc-rolling-sales.csv in same same folder were we are R and RMD File
You can also download from git hub site https://github.com/kuntalbhar/edx-nyc-project where i have uploaded the csv file nyc-rolling-sales.csv

## Data Import

We import downloaded NYC Property Sales downloaded csv file from site (file-> nyc-rolling-sales.csv) 

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
#reading from the csv. Used fread of data.table for faster reading of the csv file 
nyc_orig_data <- as_data_frame(fread("nyc-rolling-sales.csv"))
class(nyc_orig_data)
```


```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#dimension of the read file
dim(nyc_orig_data)
```
NYC Property Sales Data has 84548 observations and 22 variables.


Column names in the file
```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#show column name present in the file
names(nyc_orig_data)
```

## Data Cleaning

First column "V1" has observation number which we can ignore. Column "BUILDING CLASS CATEGORY" has category number and title together which will split into 2 column "BUILDING CLASS CATEGORY NUMBER" and "BUILDING CLASS CATEGORY" Removed "EASE-MENT" column that has only blank. 'SALE DATE' also has date and time together we remove the time part. Add a another column "BUILDING AGE" which provde the age of the building which we will use later for calculation. This column will have age caluclated based on latest year.

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#Removed the first column 'V1' 
nyc_curr_data<- as_data_frame(nyc_orig_data[,-1])

#Separate the columns in 'BUILDING CLASS CATEGORY'
nyc_curr_data <- nyc_curr_data %>%
  separate(col = "BUILDING CLASS CATEGORY", into = c("BUILDING CLASS CATEGORY NUMBER",
                                                     "BUILDING CLASS CATEGORY"), sep = 3) %>%
  separate(col = "SALE DATE", into = c("SALE DATE", "TIME"), sep = " ")

#Removed columns - 'EASE-MENT', 'TIME' (column #8,23)
nyc_curr_data <- nyc_curr_data[,c(-8,-23)]

#adding column BUILDING AGE
nyc_curr_data <- nyc_curr_data %>% 
  mutate(`BUILDING AGE` = 2017 - `YEAR BUILT`)

#display column 
names(nyc_curr_data)
```
For further dataset refrence nyc_curr_data as current dataset and nyc_orig_data as original dataset..

```{r dup_ovg, echo = TRUE}
#Removing the Duplicates in the data frame, nyc_property 
nyc_curr_data %>% filter(duplicated(nyc_curr_data) == TRUE) %>% nrow()
```
Checked and remove the 765 duplicates entries in the data if any


```{r unique_obv, echo = TRUE}
#Removing the Duplicates in the data frame, nyc_property 
nyc_curr_data <- unique(nyc_curr_data)
dim(nyc_curr_data)
```
Dataset contains 83783 observations and 22 variables.


### Column Type Conversion 
Column types imported are "int" and "chr" whic is not helpful for our analysis. So we change the column data types to it appropriate types based on data. 

Current structure is shown below
```{r types_obv, echo = TRUE}
  str(nyc_curr_data)
```

The "BOROUGH" are converted into Factor and the Id's (1-5) are change to name as mentiond in the project detail content. BUILDING CLASS CATEGORY,TAX CLASS AT PRESENT, TAX CLASS AT TIME OF SALE, were converted into Factor's. BLOCK, LOT, ADDRESS, APARTMENT NUMBER, ZIP CODE were made Char data types. LAND SQUARE FEET, GROSS SQUARE FEET, YEAR BUILT, SALE PRICE were coverted into Numeric data types. SALE DATE was converted into a Date and format (mdy).

```{r types_conversion, echo = TRUE,  message = FALSE, warning = FALSE, eval = TRUE}
#Data Type conversions to the Data set

#factor conversion
colPos <- c(1,3,4,5,8,11,18,19)
nyc_curr_data %<>% mutate_at(colPos, funs(factor(.)))

#change id's to name of BOROUGH column
levels(nyc_curr_data$BOROUGH) <- c("Manhattan", "Bronx", "Brooklyn", "Queens", "Staten Island")

#numeric conversion
num <- c(15,16,17,20)
nyc_curr_data %<>% mutate_at(num, funs(as.numeric(.)))

#char conversion
chr <- c(6,7)
nyc_curr_data %<>% mutate_at(chr, funs(as.character(.)))

#date conversion
nyc_curr_data$`SALE DATE` <- ymd(nyc_curr_data$`SALE DATE`)

#display modified structure
str(nyc_curr_data)
```


### Zeros And Missing Values  

Because of column data types conversation it may have introduce NA on values "-"
```{r types_na, echo = TRUE,  message = FALSE, warning = FALSE, eval = TRUE}
#number observations column  introduce NA's on data types conversation
sum(is.na(nyc_curr_data))
#List of  introduce NA's on data types conversation
colSums(is.na(nyc_curr_data))
```
From above we found that 3 column has NA's. After further analysing, we found that these column lost zero values due to data type convesion to numeric. 

For example we compare "SALE PRICE" values in original data to modified data
```{r types_dash, echo = TRUE}
#compare it with SALE PRICE original data on - and 0's
sum(nyc_orig_data$`SALE PRICE` == "-") 
sum(is.na(nyc_curr_data$`SALE PRICE`)) 

sum(nyc_orig_data$`SALE PRICE` == 0) 
nyc_curr_data %>% filter(`SALE PRICE` == 0) %>% nrow() 
```

Looking at the results of column SALE PRICE, count in the original dataset had 14561 observations for ‘-’ values, After converting the data type, the current dataset has 14176 NAs. The current dataset has less counts than the original. We may suspect it may be because of blanks.

Similarly results of column SALE PRICE shows in 0's case, count in the original dataset had 10228 observations for ‘-’ values, After converting the data type, the current dataset has 10012 NAs. This conclude we have not lost any zeros or missing values during type conversion. Note that this data set has both NA values and 0 values. Simmilary the same thing happen for all the numuric column which LAND SQUARE FEET and GROSS SQUARE FEET.

#### Lets look at columns SALE PRICE,LAND SQUARE FEET and GROSS SQUARE FEET
SALE PRICE

Since this column contains observations 10012  that are 0 values and 14176 that are missing values we will analyze it seperately. We create 2 dataset defect dataset nyc_defect_data that will have missing and 0's and and nyc_clean_data without them. 

Dataset with zero's and missing values contains
```{r sp_defect, echo = TRUE}
# Missing & Zero values in 'Sale Price' - 28.86% of the original dataset
nyc_defect_data <- nyc_curr_data %>% filter(nyc_curr_data$`SALE PRICE` == 0 | is.na(nyc_curr_data$`SALE PRICE`))
nyc_defect_data  %>% nrow() 
```

Dataset without zero's and missing values contains
```{r clean_data, echo = TRUE}
# Base Dataset - (Sale Price = 0 or NULL) - 71.13% of original dataset. nyc_clean_data dataset will be usesfor doing the analysis requiring Sale price values.
nyc_clean_data <- nyc_curr_data %>% filter((!nyc_curr_data$`SALE PRICE` == 0) & !is.na(nyc_curr_data$`SALE PRICE`))
nyc_clean_data %>% nrow() 
```
This clean data will be use for doing the analysis requiring Sale price values.

LAND SQUARE FEET and  GROSS SQUARE FEET
Will evalute 0's and missing values on these column. one assumprtion is that 0 Sq feet could mean the property sold as land only.  

```{r sqf_cleaning, echo = TRUE}
#get the count of zerow's observation on column LAND SQUARE FEET and  GROSS SQUARE FEET
nyc_clean_data %>% filter(nyc_clean_data$`LAND SQUARE FEET` == 0 | nyc_clean_data$`GROSS SQUARE FEET` == 0) %>% nrow()
```
There are 8674 rows in clean dataset where Land Sq Footage or Gross Sq Footage is 0.

Lets check the summary of BUILDING CLASS CATEGORY where these column are 0's.
```{r sum_bcc, echo = TRUE}
#get the summary of BUILDING CLASS CATEGORY with zero's observation on LAND SQUARE FEET and GROSS SQUARE FEET
sqf_data <- nyc_clean_data %>% filter(nyc_clean_data$`LAND SQUARE FEET` == 0 | nyc_clean_data$`GROSS SQUARE FEET` == 0)
summary(sqf_data$`BUILDING CLASS CATEGORY`)
```
From above we gather observations where LAND SQUARE FEET or GROSS SQUARE FEET are 0's, we can see that these properties are of different categories. Hence, the assumption of 0 sq footage being land only  is wrong. Hence, 0 and missing values are equivalent in this dataset.

Number of observations of zero and NA in LAND SQUARE FEET
```{r lsf_count1, echo = TRUE}
#get the count of zero and NA observations on column LAND SQUARE FEET 
nyc_clean_data %>% filter(nyc_clean_data$`LAND SQUARE FEET` == 0 | is.na(nyc_clean_data$`LAND SQUARE FEET`)) %>% nrow()
```

Number of observations of zero and NA in GROSS SQUARE FEET
```{r lsf_count2, echo = TRUE}
#get the count of zero and NA observations on column GROSS SQUARE FEET 
nyc_clean_data %>% filter(nyc_clean_data$`GROSS SQUARE FEET` == 0 | is.na(nyc_clean_data$`GROSS SQUARE FEET`)) %>% nrow()
```

As these are equivalent, we convert the zero to NA for column LAND SQUARE FEET and GROSS SQUARE FEET and applying this on current and clean dataset.

```{r sqf_convert_na, echo = TRUE}
nyc_data <- nyc_clean_data 

# converting zero to NA for column LAND SQUARE FEET
nyc_data$`LAND SQUARE FEET`[nyc_data$`LAND SQUARE FEET` == 0] <- NA

# converting zero to NA for column LAND SQUARE FEET
nyc_data$`GROSS SQUARE FEET`[nyc_data$`GROSS SQUARE FEET` == 0] <- NA
```

#### Lets look at columns RESIDENTIAL UNITS, COMMERCIAL UNITS and TOTAL UNITS

Though we dont have missing values in these columns, there are 18634, 56819, 16589 zero values individually.
```{r zero_units_cnt, echo = TRUE}
#get the count of zero observations on column GRESIDENTIAL UNITS 
nyc_clean_data %>% filter(nyc_clean_data$`RESIDENTIAL UNITS` == 0) %>% nrow()

#get the count of zero observations on column COMMERCIAL UNITS 
nyc_clean_data %>% filter(nyc_clean_data$`COMMERCIAL UNITS` == 0) %>% nrow()

#get the count of zero observations on column GRESIDENTIAL UNITS 
nyc_clean_data %>% filter(nyc_clean_data$`TOTAL UNITS` == 0) %>% nrow()
```

We will analyse this seperately as we assume that the TOTAL UNITS = RESIDENTIAL UNITS + COMMERCIAL UNITS.As expected, for 58801 rows that is 98% of the clean dataset it holds. 

```{r not_match_units, echo = TRUE}
nyc_clean_data %>% filter(nyc_clean_data$`TOTAL UNITS` != nyc_clean_data$`RESIDENTIAL UNITS` +
                              nyc_clean_data$`COMMERCIAL UNITS`) %>% nrow()
```

For 794 rows where it doesn’t seem to match, TOTAL UNITS = 1 in a majority of the observations and it might be a subtitute for these columns.

Lets now look at Categorical Variables
Lets look at CATEGORAL columns

```{r zero_cat_count, echo = TRUE}
#get the count of zero observations on column GRESIDENTIAL UNITS 
nyc_clean_data %>% filter(nyc_clean_data$`YEAR BUILT` == 0) %>% nrow()

#get the count of zero observations on column COMMERCIAL UNITS 
nyc_clean_data %>% filter(nyc_clean_data$`COMMERCIAL UNITS` == 0) %>% nrow()

#get the count of zero observations on column GRESIDENTIAL UNITS 
nyc_clean_data %>% filter(nyc_clean_data$`TOTAL UNITS` == 0) %>% nrow()
```

Below table shows the category columns with number of levels (include blanks or 0) and number of missing or zero observations. We also view categories of the each columns
```{r df_level_missing, echo = FALSE}
#print levels and missing or zero observations for category columns
Column<-c("BOROUGH",
          "TAX CLASS AT PRESENT",
          "BUILDING CLASS CATEGORY NUMBER",
          "BUILDING CLASS AT PRESENT", 
          "BUILDING CLASS AT TIME OF SALE",
          "ZIP CODE",
          "YEAR BUILT"  )
Level<-c(nlevels(nyc_data$`BOROUGH`),
          nlevels(nyc_data$`TAX CLASS AT PRESENT`),
          nlevels(nyc_data$`BUILDING CLASS CATEGORY NUMBER`),
          nlevels(nyc_data$`BUILDING CLASS AT PRESENT`),
          nlevels(nyc_data$`BUILDING CLASS AT TIME OF SALE`),
          nlevels(nyc_data$`ZIP CODE`),
          n_distinct(nyc_data$`YEAR BUILT`)
        )


MissingOrZero<-c(sum(nyc_data$`BOROUGH`==""),
          sum(nyc_data$`TAX CLASS AT PRESENT`==""),
          sum(nyc_data$`BUILDING CLASS CATEGORY NUMBER`==""),
          sum(nyc_data$`BUILDING CLASS AT PRESENT`==""),
          sum(nyc_data$`BUILDING CLASS AT TIME OF SALE`==""),
          sum(nyc_data$`ZIP CODE`==0),
          sum(nyc_data$`YEAR BUILT`==0)
        )

df <- data.frame(Column, Level, MissingOrZero)
df %>% knitr::kable()

#Print the levels
"TAX CLASS AT PRESENT"
levels(nyc_data$`TAX CLASS AT PRESENT`)

"BUILDING CLASS CATEGORY NUMBER"
levels(nyc_data$`BUILDING CLASS CATEGORY NUMBER`)

"BUILDING CLASS AT PRESENT"
levels(nyc_data$`BUILDING CLASS AT PRESENT`)

print("BUILDING CLASS AT TIME OF SALE")
levels(nyc_data$`BUILDING CLASS AT TIME OF SALE`)

print("ZIP CODE")
levels(nyc_data$`ZIP CODE`)

print("YEAR BUILT")
unique(nyc_data$`YEAR BUILT`)

```

## Data Details

Below table shows final cleaned and structured dataset's columns and their datatype and description. Descriptions are taken from provided GLOSSARY OF TERMS link https://www.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf    

```{r data_details, echo = FALSE}
variable.type <- lapply(nyc_curr_data, class)
variable.description <- c("The name of the borough in which the property is located", 
"Neighbourhood name", "Building class category code to identify similar properties", 
"Building class category title to identify similar properties", 
"Assigned tax class of the property in the city - Classes 1, 2, 3 or 4", 
"Sub-division of the borough on which real properties are located", 
"Sub-division of a Tax Block, used to uniquely represent the property location",
"Used to describe a property's constructive use", "Property's street address", 
"Property's apartment number", "Property's postal code", 
"Number of residential units at the listed property",
"Number of commercial units at the listed property",
"Total number of units at the listed property", 
"Land area of the property listed in square feet", 
"Total area of all the floors of a building", "Property's construction year", 
"Assigned tax class of the property in the city at the time of sale",
"Used to describe a property's constructive use at the time of sale",
"Price paid for the property", "Date of property sale", "Age of the Building")

variable.name <- colnames(nyc_data)

nyc_datadesc <- as_data_frame(cbind(variable.name, variable.type, variable.description))
colnames(nyc_datadesc) <- c("Column Name","Data Type","Column Description")
kable(nyc_datadesc)
```

# ANALYSIS/METHODS

We uses various section under this section to analyze NYC Property Sales data. We uses 3 main analysis section 
1. Numerical Columns -> explores each Numerical Column. 
2. Visualization -> where we view the data and there relations of various columns.
3. Prediction -> This does predective  analysis where its uses varios methods like corellations, single or multi regression.

## Descriptive Analysis

Here we explore the columns SALE PRICE, LAND SQUARE FEET, GROSS SQUARE FEET, RESIDENTIAL UNITS , COMMERCIAL UNITS, TOTAL UNITS and  BUILDING AGE. We will understand further on data distrbution in these columns.

Lets check the clean current dataset that we have created.
```{r da_dim, echo = TRUE}
dim(nyc_data)
```
The data set has 59595 rows and 22 column. 



#### SALE PRICE 
Lets apply the quintal function to understand the data distribution
```{r da_quantile, echo = TRUE}
#apply the quantile function to get the distribution
quantile(nyc_data$`SALE PRICE`, probs = seq(from = 0, to = 1, by = .1))
```

```{r da_sp_1000, echo = TRUE}
nyc_data %>% filter(`SALE PRICE` <= 1000) %>% nrow()
```
As we have remove defective value earlier, we will now check on sale price less than equal to 1000 dollars which looks like low data points. There are 1125 observations with Sale Price <= $1000. We will treat it as faulty. Will add these values into defect dataset and updated dataset.

```{r da_sp_qintle, echo = TRUE}
#Adding these rows to deffect datse
nyc_data <-  nyc_curr_data %>% filter(`SALE PRICE` <= 1000) %>% 
  bind_rows(nyc_data)

#Update current dataset
nyc_data <- nyc_data %>% filter(!`SALE PRICE` <= 1000)

#apply quantile to check this data distribution
quantile(nyc_data$`SALE PRICE`, probs = seq(from = 0, to = 1, by = .1))
```
Though its created little bit better distribution but you can still find data haveing values like 2210000000 and will also have low outliers  

#### LAND SQUARE FEET
We will apply both summary and quintal to check the data distribution

```{r da_lsf_qintle, echo = TRUE}
# get the summary
summary(nyc_data$`LAND SQUARE FEET`)
#apply quantile to check this data distribution
quantile(nyc_data$`LAND SQUARE FEET`, probs = seq(from = 0, to = 1, by = .1), na.rm = TRUE)
```
 From above distrbution value we can guess above 500,000sq looks like commercial building, commercial land, factories etc.
 
#### GROSS SQUARE FEET
We will apply both summary and quintal to check the data distribution

```{r da_gsf_qintle, echo = TRUE}
# get the summary
summary(nyc_data$`GROSS SQUARE FEET`)
#apply quantile to check this data distribution
quantile(nyc_data$`GROSS SQUARE FEET`, probs = seq(from = 0, to = 1, by = .1), na.rm = TRUE)
```
As expected, over 500,000sq properties are for comercial purpose. 

#### RESIDENTIAL UNITS , COMMERCIAL UNITS and TOTAL UNITS

We will apply both summary and check for 5 RESIDENTIAL UNITS. Will look into our asumption that TOTAL UNITS = RESIDENTIAL UNITS +COMMERCIAL UNITS.


```{r da_ru_qintle, echo = TRUE}
# get the summary RESIDENTIAL UNITS
summary(nyc_data$`RESIDENTIAL UNITS`)
#lets check for 5 units
nyc_data %>% filter(`RESIDENTIAL UNITS` > 5) %>% nrow()

```


We will apply both summary and check for 5 COMMERCIAL UNITS

```{r da_cu_summ_chk, echo = TRUE}
# get the summary RESIDENTIAL UNITS
summary(nyc_data$`COMMERCIAL UNITS`)
#lets check for 5 units
nyc_data %>% filter(`COMMERCIAL UNITS` > 5) %>% nrow()
```

```{r da_tu_qintle, echo = TRUE}
#lets check for 5 units
nyc_data %>% filter(`TOTAL UNITS` > 5) %>% nrow()

```
This proof Total Units (1573) is not equal to Residential and Commericial Units (1376 + 140= 1516).  As Total Units has NAs, we will be using this field for further analysis.

#### BUILDING AGE
There are 4195 properties with Building age. When we remove properties that don’t have a Year Built entry or Year Built = 0, we get 28 property details.

```{r da_bg_analysis, echo = TRUE}
# get the summary BUILDING AGE
summary(nyc_data$`BUILDING AGE`)
# get the summary YEAR BUILT
summary(nyc_data$`YEAR BUILT`)

#number of BUILDING AGE properties over the mean age
nyc_data %>% filter(`BUILDING AGE` > 205) %>% nrow()

#number of BUILDING AGEproperties over the mean age and YEAR BUILT is not NA and Zero
nyc_data %>% filter(`BUILDING AGE` > 205 & `YEAR BUILT` != 0) %>% 
  arrange(desc(`BUILDING AGE`)) %>% nrow()

```

## Visualization Analysis

This explore the relationship between the Numerical and Category columns by visualizing the fields.

#### BOROUG
We plot most property sales in each brough

```{r da_bg_sum_saleprice, echo = TRUE}

ggplot(data = nyc_curr_data, aes(x = `BOROUGH`)) +
  geom_bar() +
  ggtitle("Most In-Demand Borough in NYC", subtitle = "Borough-wise # of property sales in NYC") +
  scale_y_continuous("# of Property Sales", labels = scales::comma) +
  scale_x_discrete("Borough")
```

We plot average property sales in each brough
```{r da_bg_avg_saleprice, echo = TRUE}
 ggplot(data = nyc_data, aes(x = `BOROUGH`, y = mean(`SALE PRICE`) )) +
  geom_bar(stat = "identity") +
  ggtitle("Most Expensive Borough in NYC", subtitle = "Borough-wise Avg Property Sale Price in NYC") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Borough")
```
The Average Sale Price of a property in Queens is higher than Manhattan though Manhattan cost price is more.

#### NEIGHBORHOOD

We do furthe analysis to understand the price of neighborhood inside Borough. We look into Number of Sales the most in-demand neighborhoods. We look for top 15 neighborhoods

```{r demand_neighbourhood, echo = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
#create data frame based on descending order
df1 <- as.data.frame(table(nyc_curr_data$BOROUGH, nyc_curr_data$NEIGHBORHOOD))
names(df1) <- c('BOROUGH','NEIGHBORHOOD', 'Freq')
df1 <- df1 %>% arrange(desc(Freq)) %>% head(15)

#plot Top Neighborhood by sales price
ggplot(df1, aes(x = `NEIGHBORHOOD`, y = `Freq`, fill = `BOROUGH`)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Most-in demand Neighborhood in NYC", subtitle = "Top Neighborhoods by Number") +
  theme(legend.position = "bottom") +
  scale_y_continuous("# of Sales", labels = scales::comma) +
  scale_x_discrete("Neighborhood") 

```
Top Neighborhoods by # of Property Sales plot shows that neighborhoods in Queens and Manhattan had the most number of properties sold and this accounted to 11 out of the 15 top neighborhoods.

We plot Average property Sales Prices across the most in-demand neighborhoods.
```{r avg_neighbourhood, echo = TRUE, message=FALSE, warning=FALSE}
##filter data based on descending order
df2 <- 
  nyc_data %>% group_by(BOROUGH, NEIGHBORHOOD) %>% 
  summarise(MeanSP = mean(`SALE PRICE`)) %>% 
  arrange(desc(MeanSP)) %>% head(15)

#plot Most Expensive Neighborhoods by Avg Price
ggplot(data = df2, aes(x = `NEIGHBORHOOD`, y = MeanSP, fill = `BOROUGH`)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(legend.position = "bottom") +
  ggtitle("Most Expensive Neighborhoods in NYC", 
          subtitle = "Top Neighborhoods by Avg Price") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Neighborhood") 

```
The avg. price of the Most expensive Neighborhood and the second most expensive by $12.5 billion. Needless to say, the standard deviation of the property prices in NYC is large. Also interesting to note is that 11 out the 15 top property value neighborhoods are from Manhattan. Also even though no neighborhood in Queens fetched top bucks, it sold much more properties than the other boroughs. Staten Island show the biggest sale of 46 millon.

We plot least Sales Prices across the most in-demand neighborhoods. 
```{r least_neighbourhood, echo = TRUE, message=FALSE, warning=FALSE}
##filter data 
df2 <- 
  nyc_data %>% group_by(BOROUGH, NEIGHBORHOOD) %>% 
  summarise(MeanSP = mean(`SALE PRICE`)) %>% 
  arrange(MeanSP) %>% head(15)

#plot Top Neighborhoods by the lowest avg. Price
ggplot(data = df2, aes(x = `NEIGHBORHOOD`, y = MeanSP, fill = `BOROUGH`)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(legend.position = "bottom") +
  ggtitle("Least Expensive Neighborhoods in NYC", subtitle = "Top Neighborhoods by the lowest avg. Price") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Neighborhood") 

```

Interesting to note is the differences in the Avg Sale Price scale of the Most and the Least expensive properties in NYC. The avg property price in Van Cortlandt Park in the Bronx sold for dollar 160,000, while the most expensive property in Staten Island, Bloomsfield sold for $46 billion.

#### BUILDING CLASS CATEGORY
We will try to find which type of building is sold across all the Borough in New York

We plot Most In-Demand Buildings and Expensive in NYC by Borough
```{r bld_in_demand, echo = TRUE, message=FALSE, warning=FALSE}
##filter data 
df1 <- as.data.frame(table(nyc_curr_data$BOROUGH, nyc_curr_data$`BUILDING CLASS CATEGORY`))
names(df1) <- c('BOROUGH','BUILDING CLASS CATEGORY', 'Freq')
df1 <- df1 %>% group_by(BOROUGH) %>% arrange(desc(Freq)) %>% head(10)

##plot Most In-Demand Building Type class by Borough
ggplot(df1, aes(x = `BOROUGH`, y = `Freq`, fill = `BUILDING CLASS CATEGORY`)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("Most In-Demand Building Type's in NYC by Borough", subtitle = "Top Building Type's sold in NYC") +
  scale_y_continuous("# of Sales", labels = scales::comma) +
  scale_x_discrete("Borough") 
```
From above we see that most one family dwellings sold in queens, Staten Island and Brooklyn. Similarly Coops in Elevator Apartments are also wanted in Manhatttan, Queens and Brooklyn.

Now lets plot Most Expensive Buildings by value in NYC
```{r bld_in_value, echo = TRUE, message=FALSE, warning=FALSE}
##filter data 
df2 <- 
  nyc_data %>% group_by(BOROUGH, `BUILDING CLASS CATEGORY`) %>% 
  summarise(MeanSP = mean(`SALE PRICE`)) %>% 
  arrange(desc(MeanSP)) %>% head(10)

#plot Most Expensive Building Types by Borough
ggplot(data = df2, aes(x = `BUILDING CLASS CATEGORY`, y = MeanSP, fill = `BOROUGH`)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(legend.position = "bottom") +
  ggtitle("Most Expensive Building Types in NYC", subtitle = "Top Property Type's by Value in NYC") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Building Type") 
```

We see Manhattan mostly has commercial buildings and are expensive. The apartments and condos are expensive but cheaper side.  

Lets look at Least expensive Buildings in NYC
```{r bld_least, echo = TRUE, message=FALSE, warning=FALSE}
##filter data 
df2 <- 
  nyc_data %>% group_by(BOROUGH, `BUILDING CLASS CATEGORY`) %>% 
  summarise(MeanSP = mean(`SALE PRICE`)) %>% 
  arrange(MeanSP) %>% head(10)

#plot Least Expensive Building Types by Borough
ggplot(data = df2, aes(x = `BUILDING CLASS CATEGORY`, y = MeanSP, fill = `BOROUGH`)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  theme(legend.position = "bottom") +
  ggtitle("Least Expensive Buildings in NYC", subtitle = "Lowest Types of Property by Value in NYC in 2016") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Building Type") 
```
Most expensive and the least expensive buildings in NYC are commercial buildings. Intrestingly the least expensive property in NYC is a Condo Parking space in Staten island and $45,000 you could also buy a Condo Terrace in the Queens.


#### BOROUGH BY TAX CLASS

There are 4 type of class defined in Glossary of Terms 
Class 1: Includes most residential property of up to three units
Class 2: Includes all other property that is primarily residential, such as cooperatives and condominiums.
Class 3: Includes property with equipment owned by a gas, telephone or electric company
Class 4: Includes all other properties not included in class 1,2, and 3, such as offices, factories, warehouses, garage buildings, etc.

Now let’s look at the Tax Class wise of the Properties sold in NYC.
```{r bld_tax, echo = TRUE, message=FALSE, warning=FALSE}
#plot nuber of Property Tax Class wise property by Borough
ggplot(data = nyc_curr_data, aes(x = `TAX CLASS AT TIME OF SALE`)) +
  geom_bar() +
  facet_wrap(~ BOROUGH) + 
  ggtitle("Borough-wise Sold Property Tax Class") +
  scale_y_continuous("# of Property Sold", labels = scales::comma) +
  scale_x_discrete("Tax Class")
```

We now plot Avg Property Price by Tax Class sold in NYC
```{r bld_tax_avg, echo = TRUE, message=FALSE, warning=FALSE}
#plot value of Property Tax Class wise property by Borough
ggplot(data = nyc_data, aes(x = `TAX CLASS AT TIME OF SALE`, y = mean(`SALE PRICE`))) +
  geom_bar(stat = "identity") +
  facet_wrap(~ BOROUGH) + 
  ggtitle("Borough-wise Sold Property Tax Class") +
  scale_y_continuous("Avg Property Value", labels = scales::dollar) +
  scale_x_discrete("Tax Class")
```
Queens has a maximum number of lower-valued small sized residential properties while Manhattan has a large number of high-value residential condos and coops.

####  PROPERTY BY SQUARE FEET (LAND and GROSS)

To analyze the size of the property

Lets look at Land Square Feet across Boroughs
```{r sqf_lsf, echo = TRUE, message=FALSE, warning=FALSE}
#plot Land Square Feet across Boroughs
ggplot(data = nyc_data, aes(x = `BOROUGH`, y = log(`LAND SQUARE FEET`), fill = nyc_data$BOROUGH)) +
  geom_boxplot() +
  theme(legend.position = "bottom") +
  ggtitle("Land Square Feet across Boroughs", subtitle = "Borough-wise Distribution of Avg Land Square Feet") +
  scale_y_continuous("Avg. Land Square Footage", labels = scales::comma) +
  scale_x_discrete("Borough") +
  coord_flip()
```
Here we can see the outliers. These are the outliers we identified in the previous section's.  Staten Island Borough has many outliers.

We now plot Gross Square Feet across Boroughs
```{r sqf_gsf, echo = TRUE, message=FALSE, warning=FALSE}
#plot Gross Square Feet across Boroughs
ggplot(data = nyc_data, aes(x = `BOROUGH`, y = log(`GROSS SQUARE FEET`), fill = nyc_data$BOROUGH)) +
  geom_boxplot() +
  theme(legend.position = "bottom") +
  ggtitle("Gross Square Feet across Boroughs", subtitle = "Borough-wise Distribution of Avg Gross Square Feet") +
  scale_y_continuous("Abg. Gross Square Footage", labels = scales::comma) +
  scale_x_discrete("Borough") +
  coord_flip()
```
Analizing we find many property sales greater than 500,000 sq feet. Here also we can see the outliers. However, in contrast to the previous plot, a majority of these seem to be in Manhattan.


Now lets see how property size vs sale price perform.

Let see the pattern SALES PRICE VS LAND SQUARE FEET
```{r sqf_lsf_sp, echo = TRUE, message=FALSE, warning=FALSE}
#plot pattern Land Square Feet vs sales price by Boroughs
ggplot(data = nyc_data, aes(x = log(`LAND SQUARE FEET`), y = log(`SALE PRICE`), color = `BOROUGH`)) +
  geom_jitter() +
  geom_smooth(method = "lm", colour="black", size=0.5, linetype = "dashed") +
  theme(legend.position = "bottom") +
  facet_wrap(~ BOROUGH) +
  ggtitle("Price Vs Land Square Footage in NYC", 
          subtitle = "Distribution of Sale Price vs Land Square feet Borough-wise") +
  scale_y_continuous("Property Sale Price", labels = scales::dollar) +
  scale_x_continuous("Land Square Footage", labels = scales::comma) 
```

Let see the pattern SALES PRICE VS GROSS SQUARE FEET
```{r gsf_lsf_sp, echo = TRUE, message=FALSE, warning=FALSE}
#plot pattern Gross Square Feet vs sales price by Boroughs
ggplot(data = nyc_data, aes(x = log(`GROSS SQUARE FEET`), y = log(`SALE PRICE`), color = `BOROUGH`)) +
  geom_jitter() +
  geom_smooth(method = "lm", colour="black", size=0.5, linetype = "dashed") +
  theme(legend.position = "bottom") +
  facet_wrap(~ BOROUGH) +
  ggtitle("Price Vs Land Square Footage in NYC", 
          subtitle = "Distribution of Sale Price vs Land Square feet Borough-wise") +
  scale_y_continuous("Property Sale Price", labels = scales::dollar) +
  scale_x_continuous("Land Square Footage", labels = scales::comma) 
```
The trends except Manhattan seems seem for Gross Square Footage and Land Square Footage.

We also look int the maximum price per sq/feet price for neighborhood
```{r neigh_price_sq_high, echo = TRUE, message=FALSE, warning=FALSE}
#filter data

df1 <- nyc_data %>% filter(`LAND SQUARE FEET` != 0) %>%
      mutate(PriceLSF = `SALE PRICE`/`LAND SQUARE FEET`) %>%
      group_by(`BOROUGH`, `NEIGHBORHOOD`) %>%
      summarise(MeanPriceLSF = mean(PriceLSF, na.rm = TRUE)) %>%
      arrange(desc(MeanPriceLSF)) %>% head(15)

ggplot(data = df1, aes(x = `NEIGHBORHOOD`, y = MeanPriceLSF, fill = `BOROUGH`)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(legend.position = "bottom") +
  ggtitle("NYC Price per Square Feet", subtitle = "Top Price/sqft Neighborhood") +
  scale_y_continuous("Price/sqft", labels = scales::dollar) +
  scale_x_discrete("Neighborhood") 

```
As expected neighbourhoods in borough Manhattan has highest price per square feet in top 15

We also look int the minimum price per sq/feet price for neighborhood
```{r neigh_price_sq_low, echo = TRUE, message=FALSE, warning=FALSE}
#filter data

df2 <- nyc_data %>% filter(`LAND SQUARE FEET` != 0) %>%
  mutate(PriceLSF = `SALE PRICE`/`LAND SQUARE FEET`) %>%
  group_by(`BOROUGH`, `NEIGHBORHOOD`) %>%
  summarise(MeanPriceLSF = mean(PriceLSF, na.rm = TRUE)) %>%
  arrange(MeanPriceLSF) %>% head(15)

ggplot(data = df2, aes(x = `NEIGHBORHOOD`, y = MeanPriceLSF, fill = `BOROUGH`)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme(legend.position = "bottom") +
  ggtitle("MYC Price per Square Feet", subtitle = "Lowest Price/sqft Neighborhood") +
  scale_y_continuous("Price/ sqft", labels = scales::dollar) +
  scale_x_discrete("Neighborhood") 

```
Intersting that one neighborhood of Manhattan and majority from Staten Island falls in price per square feet in lowest 15. 

#### BUILDING AGE
Here we explore the building age relationship with borough and sales price

Now lets look at how building age distributed across borough. We have use 300 as age limit.
```{r ba_borough, echo = TRUE, message=FALSE, warning=FALSE}
#filter data
df3 <- 
  nyc_data %>%filter(`BUILDING AGE` <=300)  

#plot building age across borough
ggplot(data = df3, aes(x = `BOROUGH`, y = `BUILDING AGE`, fill = `BOROUGH`)) +
  geom_boxplot() +
  coord_flip() +
  ggtitle("Age of Properties sold in NYC", subtitle = "Distribution of building age in NYC") +
  scale_y_continuous("Building Age", labels = scales::comma) +
  scale_x_discrete("Borough") 

```
The plot shows that the new buildings are in Queens and Bronx, and the oldest building is in Manhattan.  There are some outliers where building constructed in 1820   

Now lets see how Sale Price distributed across Building age 
```{r ba_borough_sp, echo = TRUE, message=FALSE, warning=FALSE}
#plot sale price of borough across building age
ggplot(data = df3, aes(x = `BUILDING AGE`, y = log(`SALE PRICE`))) +
  geom_point(aes(col = df3$BOROUGH)) +
  geom_smooth(method = "lm") +
  theme(legend.position = "bottom") +
  ggtitle("Price of Oldest Properties sold in NYC", subtitle = "Oldest buildings in NYC in 2016") +
  scale_y_continuous("Property Value Distribution", labels = scales::dollar) +
  scale_x_continuous("Building Age") 


```


## Prediction Analysis

We will implement various methods get the to improve our  prediction

We will create a new that will needed in our prediction dataset and remove all unwanted character type columns Also change Sale date into the month. 
Now split the data into training and test set in 80-20% ratio


```{r pred_ds, echo = TRUE, message=FALSE, warning=FALSE}
library(caret)
# creating the new dataset

#transforming sale date into months 
nyc_data$`SALE MONTH` <- as.factor(months(nyc_data$`SALE DATE`))

#removing all unwanted columns that are not needed for prediction 
nyc_predtion <- nyc_data[, -c(3, 5, 6, 7, 8, 9, 10, 17, 19, 21)]
nyc_predtion$NEIGHBORHOOD <- as.factor(nyc_predtion$NEIGHBORHOOD)

nyc_predtion <- nyc_predtion[c(1:10, 12,13,11)]

str(nyc_predtion)

# split the data into training nyc_pred_train and test set nyc_pred_test in 80-20% ratio
set.seed(101)
#index <- sample(nrow(nyc_predtion),nrow(nyc_predtion)*0.80)

index = createDataPartition(nyc_predtion$`SALE PRICE`, p = 0.80, list = FALSE) 
nyc_pred_train <- nyc_predtion[index,]
nyc_pred_test <- nyc_predtion[-index,]

```


We will use all the columns/variabls that used in visualization analysis. We will use all the predictor columns and the sale price

### Method 1: Correlation

Corealtion Table 1

We create corealtion and plot table  below 
```{r corelate_table, echo = TRUE, message=FALSE, warning=FALSE}
library(corrplot)
#compute correlation  
distance_corr <- vapply(nyc_pred_train[c(1:12)], 
                        function(x) { cor(nyc_pred_train$`SALE PRICE`, as.numeric(x), use = "pairwise.complete.obs") }, 
                        FUN.VALUE = numeric(1))
effect_corr <- vapply(distance_corr, function(x) { ifelse(x >= 0, "Positive", "Negative")}, 
                      FUN.VALUE = character(1))
#get the name
var_corr <- names(nyc_pred_train[c(1:12)])

# create correlation table
table1 <- data.frame(var_corr, abs(distance_corr), effect_corr)
table1 <- table1[order(-abs(distance_corr)),]
names(table1) <- c("Column/Variable", "Correlation Size", "Correlation Effect")
kable(table1)
```

From above correlation matrix table only variable Gross Square feet that we need to check the statistical significance


Corealtion Table 2

Now look at the correlation between the categorical predictors in the data chart and table below
```{r corelate_num_chart2, echo = TRUE, message=FALSE, warning=FALSE}
#applying the TOTAL UNITS instead of RESIDENTIAL UNITS and COMMERCIAL UNITS. And LAND SQUARE FEET instead of LAND SQUARE FEET and GROSS SQUARE FEET
temp <- nyc_predtion[,c(1:4,10,12)]    
num <- c(1:6)
temp %<>% mutate_at(num, funs(as.numeric(.)))
#plot table of correlations
corrplot(cor(temp), type = "upper",order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

Now lets look at the table
```{r corelate_num_tabls2, echo = TRUE, message=FALSE, warning=FALSE}
#plot table of correlations
round(cor(temp), 3)
```
From above here are the observation 
BOROUGH and ZIP CODE are highly correlated 0.65 
BUILDING CLASS CATEGORY, TAX CLASS AT TIME OF SALE are highly correlated 0.613


Corealtion Table 3

Now look at the correlation between the numeric predictors in the data chart and table below
```{r corelate_num_chart3, echo = TRUE, message=FALSE, warning=FALSE}
#Correlate numeric predictors
correl <- cor(nyc_predtion[sapply(nyc_predtion, is.numeric)], use = "pairwise.complete.obs")
#plot the corelations chart
corrplot(correl, type = "upper",order = "AOE", 
         tl.col = "black", tl.srt = 45)
```

Now lets look at the table
```{r corelate_num_table3, echo = TRUE, message=FALSE, warning=FALSE}
#plot table of correlations
round(correl, 3)
```
From above here are the observation 
COMMERCIAL UNITS and TOTAL UNITS are nearly correlated 0.577  
RESIDENTIAL UNITS, TOTAL UNITS, GROSS SQUARE FEET are highly correlated approx >.7
LAND SQUARE FEET and GROSS SQUARE FEET are highly correlated 0.664
We can consider using TOTAL UNITS instead of RESIDENTIAL UNITS and COMMERCIAL UNITS almost close to 90% of the data
Same for LAND SQUARE FEET instead of LAND SQUARE FEET and GROSS SQUARE FEET. 


### Method 2: Single Linear Regressions

We will check here single factor linear regrection and find out how our predection working and if they are nessary. As we know there are many categorical variables running full regression doesnt seem correct as it will create too many dummy variables. We will maintain 4 dummy variable

We apply regresion on BOROUGH and SALE PRICE and check the summary and p-value
```{r slr_borough_model, echo = TRUE, message=FALSE, warning=FALSE} 
#apply single linear regressions SALE PRICE price with BOROUGH
#library(semEff)
slr_model <- lm(nyc_pred_train$`SALE PRICE` ~ nyc_pred_train$BOROUGH)

reg <- summary(slr_model) 

#F-statistic p-Value
print("p-value")
pf(reg$fstatistic[1],reg$fstatistic[2],reg$fstatistic[3],lower.tail = FALSE)
reg
#Adjusted R Squared 
print("Adjusted r squared")
reg$r.squared

```

Borough is significant.

We apply regresion on NEIGHBORHOOD and SALE PRICE and check the summary and p-value
```{r slr_neigh_model, echo = TRUE, message=FALSE, warning=FALSE} 
#apply single linear regressions SALE PRICE price with NEIGHBORHOOD

slr_model <- lm(nyc_pred_train$`SALE PRICE` ~ nyc_pred_train$NEIGHBORHOOD)
#print summary
reg <- summary(slr_model) 

#F-statistic p-Value
print("p-value")
pf(reg$fstatistic[1],reg$fstatistic[2],reg$fstatistic[3],lower.tail = FALSE)

#Adjusted R Squared 
print("Adjusted r squared")
reg$r.squared

```

Only 4 neighborhoods are significant. We will create manually these 4 significant neighborhoods as dummy variable and put remaining  under "all others" variable

```{r slr_neigh_4, echo = TRUE, message=FALSE, warning=FALSE}

nyc_pred_train$N_BLOOMFIELD = ifelse(nyc_pred_train$NEIGHBORHOOD == "BLOOMFIELD", 1,0)
nyc_pred_train$N_FASHION = ifelse(nyc_pred_train$NEIGHBORHOOD == "FASHION", 1,0)
nyc_pred_train$`N_JAVITS CENTER` = ifelse(nyc_pred_train$NEIGHBORHOOD == "JAVITS CENTER", 1,0)
nyc_pred_train$`N_MIDTOWN CBD` = ifelse(nyc_pred_train$NEIGHBORHOOD == "MIDTOWN CBD", 1,0)
nyc_pred_train$`N_OTHERS` = ifelse((nyc_pred_train$NEIGHBORHOOD != "MIDTOWN CBD") &
                                                       (nyc_pred_train$NEIGHBORHOOD != "JAVITS CENTER") &
                                                       (nyc_pred_train$NEIGHBORHOOD != "FASHION") &
                                                       (nyc_pred_train$NEIGHBORHOOD != "BLOOMFIELD"), 1,0)

# Removing the original Neighborhood predictor
nyc_pred_train <- nyc_pred_train[,-2]
```


We apply regresion on BUILDING CLASS CATEGORY and SALE PRICE and check the summary  and p-value
```{r slr_blc_model, echo = TRUE, message=FALSE, warning=FALSE} 
#apply single linear regressions SALE PRICE price with BUILDING CLASS CATEGORY and p-value
slr_model <- lm(nyc_pred_train$`SALE PRICE` ~ nyc_pred_train$`BUILDING CLASS CATEGORY`)
#print summary
reg <- summary(slr_model) 

#F-statistic p-Value
print("p-value")
pf(reg$fstatistic[1],reg$fstatistic[2],reg$fstatistic[3],lower.tail = FALSE)

#Adjusted R Squared 
print("Adjusted r squared")
reg$r.squared

```
Only 8 of the 46 variables created are NOT significant. So, at this point we wii leave all the 46 dummy levels created for this variable.


We apply regresion on TAX CLASS AT TIME OF SALE and SALE PRICE and check the summary  and p-value
```{r slr_tax_model, echo = TRUE, message=FALSE, warning=FALSE} 
#apply single linear regressions SALE PRICE price withTAX CLASS AT TIME OF SALE and p-value
slr_model <- lm(nyc_pred_train$`SALE PRICE` ~ nyc_pred_train$`TAX CLASS AT TIME OF SALE`)
#print summary
reg <- summary(slr_model) 
reg
#F-statistic p-Value
print("p-value")
pf(reg$fstatistic[1],reg$fstatistic[2],reg$fstatistic[3],lower.tail = FALSE)

#Adjusted R Squared 
print("Adjusted r squared")
reg$r.squared

```

All the levels of Tax Class are significant hence we main maintaining them.

We apply regresion on SALE MONTH and SALE PRICE and check the summary  and p-value
```{r slr_sm_model1, echo = TRUE, message=FALSE, warning=FALSE} 
#apply single linear regressions SALE PRICE price with SALE MONTH and p-value
slr_model <- lm(nyc_pred_train$`SALE PRICE` ~ nyc_pred_train$`SALE MONTH`)
#print summary
reg <- summary(slr_model) 

#F-statistic p-Value
print("p-value")
pf(reg$fstatistic[1],reg$fstatistic[2],reg$fstatistic[3],lower.tail = FALSE)

#Adjusted R Squared 
print("Adjusted r squared")
reg$r.squared

```
None of the levels of the sales month variable are significant we ignore this variable


### Method 3: Multi Linear Regressions

We will removd the SALE MONTH as this variable is not significant
```{r mlr_sm_model1, echo = TRUE, message=FALSE, warning=FALSE} 

attach(nyc_pred_train)
full_mlr_model <- lm(`SALE PRICE` ~ . -`SALE MONTH` , data = nyc_pred_train)
reg <- summary(full_mlr_model)
gl<- glance(full_mlr_model) 
gl

print("MSE")
mean(reg$residuals^2)

```

We will ignore the varaible ZIP CODE as we a variable reduction process We have already have process the NEIGHBORHOOD so ZIP CODE.

From above table though Adjusted R-squared value is quite high at 0.8924342,  the Model MSE is very large at 3.37343e+13 and AIC is very high

```{r mlr_sm_model2, echo = TRUE, message=FALSE, warning=FALSE} 

attach(nyc_pred_train)
full_mlr_model <- lm(`SALE PRICE` ~ . -`SALE MONTH` -`ZIP CODE`  , data = nyc_pred_train)
reg <- summary(full_mlr_model)

  
gl<- glance(full_mlr_model) 
gl

print("MSE")
mean(reg$residuals^2)

```

From above table though Adjusted R-squared value is reduce to 0.695746, though MSE is very large at 9.635337 and AIC is very high. 

We need furter analysis and predictions using different methods and alogrithms. Which includes varable reductions, cross validations, random forest etc.

# RESULT

We have clean the data, treated the missing values and identified the outliers. We have visualize the data and figure out the trend. 

Here are what we have found from our analysis:

Property prices in NYC range from Dollar 220,000 (10% percentile of Property prices) all the way to 2.2 billion (95% percentile of Property prices).

Price per square footage in Manhattan is as high as Dollar 16,000/sqft, while in Bloomfield, Staten Island is $26/sqft. 

Manhattan and Bronx sold the most residential condo apartments in large buildings/ residential societies, while Queens sold the most residential homes.

NYC has a place for everyone and you move to Staten Island with cheaper per sqft.

We need Further analyses.

# CONCLUSION

We have applied correlation of the variables and applied regressions modals. We can conclude Single Linear Regression method is not appropriate  as there are too many categorical predictors. Multi Linear Regression method the R-squared value is quite high and MSE is very large. We can continue to apply variable selection methods to get the optimal number of parameters to the modal. Apply cross-validation methods to test the out-of-sample error and try other algorithm modal like Random Forest, KNN to create optimal modal for predection

Moreover, this exploration show the relation between Sale Prices and the variables explored such as Borough, Neighborhood, Building Type, Tax Class, Building Age, Land/Gross Square Feet. 

We need further analysis by applying Random Forest.A futher extension of this study will be using KNN imputation to fill in missing values and then evaluate the regression strength. But due to hardware constrain we have done analysis till MLR methods. 

# REFERENCES

Project taken from Kaggle site metioned in EDX link
https://www.kaggle.com/code/annavictoria/ml-friendly-public-datasets/notebook?utm_medium=email&utm_source=intercom&utm_campaign=data+projects+onboarding

Project NYC Property Sales
https://www.kaggle.com/new-york-city/nyc-property-sales

The dataset was downloaded from Kaggle 
www.kaggle.com/new-york-city/nyc-property-sales

For further reference on individual fields see the Glossary of Terms  https://www.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf. 

For the building classification codes see the Building Classifications Glossary https://www.nyc.gov/assets/finance/jump/hlpbldgcode.html.




